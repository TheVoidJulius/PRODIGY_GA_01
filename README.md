# PRODIGY_GA_01

# Text Generation with GPT-2

This repository contains **Task-01** of my internship at **Prodigy Infotech**, where I fine-tuned a GPT-2 model to generate coherent and contextually relevant text using a custom dataset.

---

## ğŸ“Œ Task Objective
To train a transformer-based language model (GPT-2) that can generate human-like text based on a given prompt.

---

## ğŸ§  Technologies Used
- Python
- PyTorch
- Hugging Face Transformers
- GPT-2
- PyCharm

---

## ğŸ“ Project Structure
Prodigy_Infotech_Task-01/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ data.txt
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ train.py
â”‚ â””â”€â”€ generate.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
