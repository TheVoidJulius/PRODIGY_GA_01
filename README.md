# PRODIGY_GA_01

# Text Generation with GPT-2

This repository contains **Task-01** of my internship at **Prodigy Infotech**, where I fine-tuned a GPT-2 model to generate coherent and contextually relevant text using a custom dataset.

---

## ðŸ“Œ Task Objective
To train a transformer-based language model (GPT-2) that can generate human-like text based on a given prompt.

---

## ðŸ§  Technologies Used
- Python
- PyTorch
- Hugging Face Transformers
- GPT-2
- PyCharm

---

## ðŸ“ Project Structure
Prodigy_Infotech_Task-01/

â”œâ”€â”€ data/

â”‚ â””â”€â”€ data.txt

â”œâ”€â”€ src/

â”‚ â”œâ”€â”€ train.py

â”‚ â””â”€â”€ generate.py

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md



---

## âš™ï¸ How to Run the Project

### 1ï¸âƒ£ Install dependencies:
>>> pip install -r requirements.txt

### 2ï¸âƒ£ TRAIN THE GPT-2 MODEL:
run the below command in your terminal ->
python src/train.py

### 3ï¸âƒ£ GENERATE TEXT:
run the below command in your terminal ->
python src/generate.py

### ðŸ§ª Sample Output:

>>> Artificial Intelligence is a good example of a topic that is ripe for discussion
and highlights the importance of modern AI systems.


|--- Thank You ---|


